# -*- coding: utf-8 -*-
"""Cybersecuirity.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1D9pb9Ibe3j4UyARIufaGNmnyDVMQlIjQ
"""



from google.colab import drive
drive.mount('/content/drive')





import pandas as pd
data=pd.read_csv("/content/drive/MyDrive/Cleaned_UNSW_NB15_testing-set.csv")

!pip install transformers

!pip install torch scikit-learn pandas

from transformers import BertTokenizer, BertForSequenceClassification
import torch
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

tokenizer=BertTokenizer.from_pretrained('bert-base-uncased')

def tokenize_features(row):
    return tokenizer(
        f"{row['proto']}  {row['service']} {row['state']} {row['sbytes']} {row['dbytes']}",
        padding='max_length',
        truncation=True,
        max_length=128,
        return_tensors='pt'
    )
tokenized_data=data.apply(tokenize_features,axis=1)
lables=data['label'].values
train_texts,test_texts,train_labels,test_labels=train_test_split(tokenized_data,lables,test_size=.2,random_state=42)
model=BertForSequenceClassification.from_pretrained('bert-base-uncased',num_labels=2)

import torch
device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

# Combine relevant columns into a single string for each row
text_data = data.apply(
    lambda row: f"{row['proto']} {row['service']} {row['state']} {row['sbytes']} {row['dbytes']}",
    axis=1
).tolist()  # Convert to list of strings

# Tokenize the combined text data
tokenized = tokenizer(
    text_data,
    padding=True,
    truncation=True,
    max_length=128,
    return_tensors="pt"
)

# Extract tokenized components
input_ids = tokenized['input_ids']
attention_mask = tokenized['attention_mask']
labels = torch.tensor(data['label'].values)  # Ensure labels are in tensor format

from sklearn.model_selection import train_test_split

train_input_ids, test_input_ids, train_attention_mask, test_attention_mask, train_labels, test_labels = train_test_split(
    input_ids, attention_mask, labels, test_size=0.2, random_state=42
)

from torch.utils.data import TensorDataset

train_dataset = TensorDataset(train_input_ids, train_attention_mask, train_labels)
test_dataset = TensorDataset(test_input_ids, test_attention_mask, test_labels)

from torch.utils.data import DataLoader

train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)
test_dataloader = DataLoader(test_dataset, batch_size=16)

from transformers import BertForSequenceClassification

# Load BERT model with classification head
model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2)

import torch
device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)



from transformers import AdamW
# Define optimizer
optimizer = AdamW(model.parameters(), lr=5e-5)

# Define loss function (automatically handled during training in Hugging Face models)
# /usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of Adamw is deprecated and
# warnings.warn(
from torch.nn import functional as F
# Training parameters
epochs = 3
for epoch in range(epochs):
    print(f"Epoch {epoch + 1}")
    model.train()
    total_loss = 0
    for batch in train_dataloader:
        # Move batch to GPU
        batch = tuple(t.to(device) for t in batch)
        input_ids, attention_mask, labels = batch
        # Forward pass
        optimizer.zero_grad()
        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        logits = outputs.logits
        # Backward pass and optimization
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    print(f"Total Loss: {total_loss}")

from sklearn.metrics import classification_report
# Put model in evaluation mode
model.eval()
# Store predictions and true labels
predictions, true_labels = [], []
with torch.no_grad():
    for batch in test_dataloader:
        batch = tuple(t.to(device) for t in batch)
        input_ids, attention_mask, labels = batch
        #Forward pass
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        logits = outputs.logits
        #Convert logits to class predictions
        preds = torch.argmax(F.softmax(logits, dim=1), axis=1).cpu().numpy()
        predictions.extend(preds)
        true_labels.extend(labels.cpu().numpy())

# Print classification report
print(classification_report(true_labels, predictions, target_names=["Normal", "Anomalous"]))

# Save model
model.save_pretrained('./bert_anomaly_detector')

# Save tokenizer
tokenizer.save_pretrained('./bert_anomaly_detector')

from google.colab import files
!zip -r bert_anomaly_detector.zip ./bert_anomaly_detector
files.download('bert_anomaly_detector.zip')

